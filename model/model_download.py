# -*- coding: utf-8 -*-
"""Bản sao của Main Datathon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zdbIf0CpoOvok6kpPNPi728UybAVBafL

# Load the data
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install -U fashion-clip

import sys
# sys.path.append("fashion-clip/")
from fashion_clip.fashion_clip import FashionCLIP
import pandas as pd
import numpy as np
from collections import Counter
from PIL import Image
import requests
from io import BytesIO
import zipfile
import pickle

numberToCheckProcess = 0

# Commented out IPython magic to ensure Python compatibility.
# %%capture
fclip = FashionCLIP('fashion-clip')

# Link to the zip file
url = "https://drive.google.com/file/d/1uWVjvZvllQqYLryBHA8gL0svUKbud-nB/view"

# Extract the file ID from the link
file_id = url.split("/")[-2]

# Create the download link
download_link = f"https://drive.google.com/uc?id={file_id}"

# Download the content of the zip file
response = requests.get(download_link)
zip_data = BytesIO(response.content)

# Unzip the file
with zipfile.ZipFile(zip_data, 'r') as zip_ref:
    # Get the list of files in the zip
    file_list = zip_ref.namelist()

    # Choose the Excel file from the list (assuming only one Excel file)
    excel_file = [file for file in file_list if file.endswith('.xlsx')][0]

    # Read data from the Excel file using pandas
    df = pd.read_excel(zip_ref.open(excel_file))

df.head()

df.shape

df.columns

# Concatenate 'description' and 'color' columns with a separator
df['description'] = 'Color of product: ' + df['color'].astype(str) + '. ' + df['description']

# Drop the specified columns from the DataFrame
df = df.drop(columns=['availability', 'avg_rating', 'brand', 'color', 'currency', 'name', 'price', 'review_count', 'scraped_at', 'url'])

df = df.dropna(subset=['images'])

df = df.drop_duplicates()

df = df.reset_index(drop=True)

df.isnull().sum()

df.head(5)

"""## Text preprocessing"""

df['description'].iloc[0]

import re
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

con_appos = {
    "rep": "represent",
    "hoodie": "hooded sweatshirt",
    "tee": "t-shirt",
    "zip": "zip-up",
    "maxi": "maxi dress",
    "mini": "mini dress",
    "midi": "midi dress",
    "denim": "denim fabric",
    "chino": "chino pants",
    "cropped": "cropped length",
    "floral": "floral print",
    "striped": "striped pattern",
    "polka": "polka dot",
    "v-neck": "v-neckline",
    "crew-neck": "crew neckline",
    "slim": "slim fit",
    "loose": "loose fit",
    "bootcut": "bootcut style",
    "skinny": "skinny fit",
    "isn't": "is not",
    "it'd": "it would",
    "it'd've": "it would have",
    "it'll": "it will",
    "it'll've": "it will have",
    "it's": "it is",
    "mayn't": "may not",
    "might've": "might have",
    "mightn't": "might not",
    "mightn't've": "might not have",
    "must've": "must have",
    "mustn't": "must not",
    "mustn't've": "must not have",
    "needn't": "need not",
    "needn't've": "need not have",
    "so've": "so have",
    "so's": "so is",
    "that'd": "that would",
    "that'd've": "that would have",
    "that's": "that is",
    "there'd": "there would",
    "there'd've": "there would have",
    "there's": "there is",
    "they'd": "they would",
    "they'd've": "they would have",
    "they'll": "they will",
    "they'll've": "they will have",
    "they're": "they are",
    "they've": "they have",
    "we're": "we are",
    "to've": "to have"
}

def preprocessed_description(text):
    # Ensure the input text is a string
    text = str(text)

    # Convert the text to lowercase
    text = text.lower()

    # Change abbreviated words into full words
    pattern = re.compile(r'\b(' + r'|'.join(con_appos.keys()) + r')\b')
    text = pattern.sub(lambda x: con_appos[x.group()], text)

    # Remove special characters and numbers
    text = re.sub(r'\W', ' ', text)
    text = re.sub(r'\d+', ' ', text)

    # Remove html tags
    Tag_re = re.compile(r'<[^>]+>')
    text  = Tag_re.sub(' ', text)

    # Remove multiple spaces
    text = re.sub(r'\s+', ' ', text)

    # Remove stopwords
    stopwords_list = [word for word in stopwords.words('english') if word != 'not']
    pattern = re.compile(r'\b(' + r'|'.join(stopwords_list) + r')\b\s*')
    text = pattern.sub('', text)

    # Lemmatize words
    lemmatizer = WordNetLemmatizer()
    text = ' '.join(lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text))

    return text

# Apply preprocessing to the 'description' column
df['description'] = df['description'].apply(preprocessed_description)

df['description'].iloc[0]

"""## Images preprocessing"""

df['images'].loc[0]


def is_valid_link(link):
    try:
        response = requests.head(link)
        return response.status_code == 200
    except requests.RequestException:
        return False

print(numberToCheckProcess)
numberToCheckProcess += 1

df['images'] = df['images'].apply(lambda x: x if isinstance(x, str) else '')
df['images'] = df['images'].apply(lambda x: [link for link in x.split('~') if is_valid_link(link)])
df = df[df['images'].apply(len) > 0]
df = df.reset_index(drop=True)

df['images'].iloc[0]

print(numberToCheckProcess)
numberToCheckProcess += 1

"""# FashionCLIP pre-trained model"""

image_urls_list = df['images'].tolist()
images = []

# Iterate over each item in the image_urls_list and convert image URLs to a list
for image_urls in image_urls_list:
    images.extend(image_urls)

texts = df['description'].tolist()

# we create image embeddings and text embeddings
image_embeddings = fclip.encode_images(images, batch_size=32)
text_embeddings = fclip.encode_text(texts, batch_size=32)

# we normalize the embeddings to unit norm (so that we can use dot product instead of cosine similarity to do comparisons)
image_embeddings = image_embeddings / np.linalg.norm(image_embeddings, ord=2, axis=-1, keepdims=True)
text_embeddings = text_embeddings / np.linalg.norm(text_embeddings, ord=2, axis=-1, keepdims=True)

precision = 0

# we could batch this operation to make it faster
for index, t in enumerate(text_embeddings):
    arr = t.dot(image_embeddings.T)

    best = arr.argsort()[-5:][::-1]

    if index in best:
        precision +=1

round(precision/len(text_embeddings), 2)

"""Test querry and display 1 images"""

text_embedding = fclip.encode_text(["white t shirt"], batch_size=32)[0]

id_of_matched_object = np.argmax(np.dot(text_embedding, image_embeddings.T))
found_image_url = images[id_of_matched_object]

# Download and resize the image
response = requests.get(found_image_url)
image = Image.open(BytesIO(response.content))

fixed_height = 224
height_percent = (fixed_height / float(image.size[1]))
width_size = int((float(image.size[0]) * float(height_percent)))
image = image.resize((width_size, fixed_height), Image.NEAREST)

image

"""Test query and display top 5 images"""

text_embedding = fclip.encode_text(["black shoes"], batch_size=32)[0]

# Compute similarity scores between text embedding and image embeddings
similarity_scores = np.dot(text_embedding, image_embeddings.T)

# Sort the images based on similarity scores in descending order
sorted_indices = np.argsort(similarity_scores)[::-1]

# Retrieve the top K most similar image URLs
top_k = 5  # Number of top similar images to retrieve
top_similar_image_urls = [images[i] for i in sorted_indices[:top_k]]

top_similar_image_urls

text_embedding = fclip.encode_text(["summer outfit for woman"], batch_size=32)[0]

# Compute similarity scores between text embedding and image embeddings
similarity_scores = np.dot(text_embedding, image_embeddings.T)

# Sort the images based on similarity scores in descending order
sorted_indices = np.argsort(similarity_scores)[::-1]

# Retrieve the top K most similar image URLs
top_k = 5  # Number of top similar images to retrieve
top_similar_image_urls = [images[i] for i in sorted_indices[:top_k]]

top_similar_image_urls

import requests
from PIL import Image
import numpy as np
from io import BytesIO
from tensorflow.keras.applications.efficientnet import EfficientNetB0, preprocess_input, decode_predictions

# Load the pre-trained EfficientNetB0 model
model = EfficientNetB0(weights='imagenet')

# Classify the top similar images
top_similar_categories = []

for image_url in top_similar_image_urls:
    response = requests.get(image_url)
    image = Image.open(BytesIO(response.content))
    image = image.resize((224, 224))  # Resize the image to match the input size of the model
    image = np.array(image)
    image = np.expand_dims(image, axis=0)
    image = preprocess_input(image)

    # Make predictions on the image
    predictions = model.predict(image)
    decoded_predictions = decode_predictions(predictions, top=5)[0]  # Get top 5 predictions
    top_category = None

    for _, label, _ in decoded_predictions:
        # Map the predicted label to the desired categories
        if label in ["sandal", "slipper", "boot", "shoe"]:
            top_category = "shoes"
            break
        elif label in ["shirt", "sweater", "t-shirt", "dress", "coat", "jacket", "hoodie", "sweathearts"]:
            top_category = "clothing"
            break
        elif label in ["trouser", "jeans", "shorts"]:
            top_category = "pants"
            break
        elif label in ["cap", "hat"]:
            top_category = "hats"
            break

    if top_category is None:
        top_category = "other"

    top_similar_categories.append(top_category)

top_similar_categories

"""function that takes a text input"""

def get_top_similar_images(text):
    # Encode text to get text embedding
    text_embedding = fclip.encode_text([text], batch_size=32)[0]

    # Compute similarity scores between text embedding and image embeddings
    similarity_scores = np.dot(text_embedding, image_embeddings.T)

    # Sort the images based on similarity scores in descending order
    sorted_indices = np.argsort(similarity_scores)[::-1]

    # Retrieve the top K most similar image URLs
    top_k = 5  # Number of top similar images to retrieve
    top_similar_image_urls = [images[i] for i in sorted_indices[:top_k]]

    return top_similar_image_urls

import matplotlib.pyplot as plt

def get_top_similar_images(text):
    # Encode text to get text embedding
    text_embedding = fclip.encode_text([text], batch_size=32)[0]

    # Compute similarity scores between text embedding and image embeddings
    similarity_scores = np.dot(text_embedding, image_embeddings.T)

    # Sort the images based on similarity scores in descending order
    sorted_indices = np.argsort(similarity_scores)[::-1]

    # Retrieve the top K most similar image URLs
    top_k = 5  # Number of top similar images to retrieve
    top_similar_image_urls = [images[i] for i in sorted_indices[:top_k]]

    # Create a dictionary with URLs and their corresponding similarity scores
    top_similar_images_info = [{'image': url, 'title': 'imagee' , 'link': "https://www.google.com/"} for i, url in enumerate(top_similar_image_urls)]
    final_return = {'accessories': top_similar_images_info}
    return final_return

# text = 'black nike shoes'
# get_top_similar_images(text)

# import pickle

# pickle_out = open('trained_model.pkl','wb')
# pickle.dump(fclip, pickle_out)
# pickle_out.close

#save trained model
import pickle
pickle_out = open("fclip.pkl","wb")
pickle.dump(fclip, pickle_out)
pickle_out.close()
